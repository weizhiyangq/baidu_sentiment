{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(r'E:\\baidumatch\\sentiment\\data\\data_train.csv',encoding='gbk',delimiter=\"\\t\",header=None,index_col=0)\n",
    "data_test=pd.read_csv(r'E:\\baidumatch\\sentiment\\data\\data_test.csv',encoding='gbk',delimiter='\\t',header=None,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train\n",
    "data_train.columns=['a','b','c']\n",
    "#data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.iloc[:,0]\n",
    "dict={}\n",
    "for i in data_train.iloc[:,0]:\n",
    "    if i in dict:\n",
    "        dict[i]+=1\n",
    "    else:\n",
    "        dict[i]=1\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "industry_dict={'食品餐饮':0,'医疗服务':1,'金融服务':2,'旅游住宿':3,'物流快递':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.iloc[:,0]=data_train.iloc[:,0].map(industry_dict)#注意：不要写成data_train.iloc[:,0].map(lambda x:industry_dict),这样就将每一行都映射为industry_dict了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pinglun=data_train['b'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_cut=[]\n",
    "for i in pinglun:\n",
    "    sentence_cut=jieba.lcut(str(i))\n",
    "    if sentence_cut!='\\r\\n': #有内容且不是换行符\n",
    "        contents_cut.append(sentence_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(r'F:\\tang_data\\text_analy\\stopwords.txt',index_col=False,sep='\\t',quoting=3,names=['stopword'],encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(contents,stopwords):\n",
    "    contents_clean=[]\n",
    "    all_clean_words=[]\n",
    "    for line in contents:\n",
    "        line_clean=[]\n",
    "        for i in line:\n",
    "            if i in stopwords or i=='[' or i==']':\n",
    "                continue\n",
    "            line_clean.append(i)\n",
    "            all_clean_words.append(i)\n",
    "            #line_clean_str=' '.join(line_clean_str)\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean,all_clean_words\n",
    "stopwords=stopwords.stopword.values.tolist()\n",
    "contents_clean,all_clean_words=drop_stopwords(contents_cut,stopwords)  #all_clean_words主要用来词云显示，对于算法模型无用\n",
    "df_contents_clean=pd.DataFrame({'contents_cut_clean':contents_clean})\n",
    "df_contents_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_contents_clean.index = range(1,len(df_contents_clean) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.b=df_contents_clean.contents_cut_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.b=data_train.b.map(lambda x:' '.join(x))\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_0=Tokenizer(num_words=3000)\n",
    "token_1=Tokenizer(num_words=3000)\n",
    "token_2=Tokenizer(num_words=3000)\n",
    "token_3=Tokenizer(num_words=3000)\n",
    "token_4=Tokenizer(num_words=3000)\n",
    "\n",
    "\n",
    "train_0=data_train.loc[data_train['a'] == 0]\n",
    "token_0.fit_on_texts(train_0.b)\n",
    "train_0_seq=token_0.texts_to_sequences(train_0.b)\n",
    "x_0=sequence.pad_sequences(train_0_seq,15)\n",
    "y_0=train_0.c\n",
    "y_0_onehot=np_utils.to_categorical(y_0)\n",
    "\n",
    "\n",
    "train_1=data_train.loc[data_train['a'] == 1]\n",
    "token_1.fit_on_texts(train_1.b)\n",
    "train_1_seq=token_1.texts_to_sequences(train_1.b)\n",
    "x_1=sequence.pad_sequences(train_1_seq,15)\n",
    "y_1=train_1.c\n",
    "y_1_onehot=np_utils.to_categorical(y_1)\n",
    "\n",
    "\n",
    "train_2=data_train.loc[data_train['a'] == 2]\n",
    "token_2.fit_on_texts(train_2.b)\n",
    "train_2_seq=token_2.texts_to_sequences(train_2.b)\n",
    "x_2=sequence.pad_sequences(train_2_seq,15)\n",
    "y_2=train_2.c\n",
    "y_2_onehot=np_utils.to_categorical(y_2)\n",
    "\n",
    "\n",
    "train_3=data_train.loc[data_train['a'] == 3]\n",
    "token_3.fit_on_texts(train_3.b)\n",
    "train_3_seq=token_3.texts_to_sequences(train_3.b)\n",
    "x_3=sequence.pad_sequences(train_3_seq,15)\n",
    "y_3=train_3.c\n",
    "y_3_onehot=np_utils.to_categorical(y_3)\n",
    "\n",
    "\n",
    "train_4=data_train.loc[data_train['a'] == 4]\n",
    "token_4.fit_on_texts(train_4.b)\n",
    "train_4_seq=token_4.texts_to_sequences(train_4.b)\n",
    "x_4=sequence.pad_sequences(train_4_seq,15)\n",
    "y_4=train_4.c\n",
    "y_4_onehot=np_utils.to_categorical(y_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(output_dim=32,input_dim=3000,input_length=15))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.add(Flatten())\n",
    "model.add(SimpleRNN(units=16))\n",
    "model.add(Dense(units=250,activation='relu'))\n",
    "model.add(Dropout(0.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "adam=keras.optimizers.Adamax(lr=0.008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['categorical_accuracy'])\n",
    "#help(model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history=model.fit(x_0,y_0_onehot,batch_size=100,epochs=60,verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history=model.fit(x_1,y_1_onehot,batch_size=100,epochs=60,verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history=model.fit(x_2,y_2_onehot,batch_size=100,epochs=60,verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history=model.fit(x_3,y_3_onehot,batch_size=100,epochs=60,verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history=model.fit(x_4,y_4_onehot,batch_size=100,epochs=60,verbose=2,validation_split=0.2)\n",
    "#五个指标：0.07927  0.0948 0.87 0.95 0.9334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
